{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Next_K_Words.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Predicting Next K Words using LSTM and BERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this notebook is to explore various language models specifically LSTM based and transformer. We will explore how the size of the model effects the sequence generated. We will see both character based and word based models. The dataset used to train the model can be found here: [link](https://drive.google.com/file/d/1OxNHKbdQm03KiNFNmERPI5wt_kTmjCW0/view?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXdkhxZAXnTW"
      },
      "source": [
        "# Word Based LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbU5DRolXseI"
      },
      "source": [
        "# Importing modules\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import numpy\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG59kp7RADwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d1fba1-5532-4ddf-84c5-274f713d096e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2NR3RFFYOT8"
      },
      "source": [
        "Do basic pre processing which includes lowering etc\n",
        "Check the dataset and apply suitable preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGytNEwsG9YM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcb5005-a0aa-46b2-a253-e171137d510f"
      },
      "source": [
        "nltk.download('punkt') # For tokenizers\n",
        "nltk.download('inaugural') # For dataset\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQvfF2NjXxGj"
      },
      "source": [
        "# Load the data and preprocess data and store corpus in raw_text\n",
        "# read dataset\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/corpus.txt\", \"rt\", encoding=\"utf-8\")\n",
        "corpus=f.read()\n",
        "#print(corpus)\n",
        "\n",
        "#tokenize corpus into sentences\n",
        "sentences=sent_tokenize(corpus)\n",
        "preprocessed_corpus=\"\";\n",
        "for sentence in sentences:\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"[a-z’]+\")\n",
        "    tokenized_sentence = tokenizer.tokenize(sentence.lower())\n",
        "    if len(tokenized_sentence)!=0:\n",
        "        preprocessed_corpus+=\" \".join(tokenized_sentence)+\" \"\n",
        "        #print(tokenized_sentence)  \n",
        "        \n",
        "        \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eug68GOecM8Z"
      },
      "source": [
        "# Hyperparameters of the model\n",
        "vocab_size = 2461 # choose based on statistics\n",
        "oov_tok = '<OOV>'\n",
        "embedding_dim = 100\n",
        "padding_type='post'\n",
        "trunc_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWNBOlJ5cQym"
      },
      "source": [
        "# tokenize sentences\n",
        "raw_text=preprocessed_corpus\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts([raw_text])\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNRJDbFcdHbO"
      },
      "source": [
        "seq_length = 50\n",
        "tokens = tokenizer.texts_to_sequences([raw_text])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykfI4FrwdyJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd6f3f4-1a36-4f5f-f7f2-456224a519f1"
      },
      "source": [
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, len(tokens) - seq_length-1 , 1):\n",
        "  seq_in = tokens[i:i + seq_length]\n",
        "  seq_out = tokens[i + seq_length]\n",
        "\n",
        "  if seq_out==1: #Skip samples where target word is OOV\n",
        "    continue\n",
        "    \n",
        "  dataX.append(seq_in)\n",
        "  dataY.append(seq_out)\n",
        " \n",
        "N = len(dataX)\n",
        "print (\"Total training data size: \", N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training data size:  26494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJmGr1xId8cO"
      },
      "source": [
        "X = numpy.array(dataX)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = numpy.array(dataY)\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QPApRA-d9JV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab73cf6-2718-411e-a814-7e2eaa367000"
      },
      "source": [
        "# with embedding\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 100)           246100    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 128)               84480     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2461)              317469    \n",
            "=================================================================\n",
            "Total params: 648,049\n",
            "Trainable params: 648,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14ClAAYpeCVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796c0198-6062-41b9-beaa-dcbed11b902d"
      },
      "source": [
        "# Use validation split of 0.2 while training\n",
        "model.fit(X, y, epochs= 100, batch_size=128, validation_split=0.2 ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "166/166 [==============================] - 8s 46ms/step - loss: 6.3936 - accuracy: 0.0546 - val_loss: 6.0758 - val_accuracy: 0.0861\n",
            "Epoch 2/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 5.9418 - accuracy: 0.0559 - val_loss: 5.9914 - val_accuracy: 0.0861\n",
            "Epoch 3/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 5.8320 - accuracy: 0.0592 - val_loss: 5.9853 - val_accuracy: 0.0878\n",
            "Epoch 4/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 5.7421 - accuracy: 0.0648 - val_loss: 5.9649 - val_accuracy: 0.0945\n",
            "Epoch 5/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 5.6534 - accuracy: 0.0706 - val_loss: 5.9294 - val_accuracy: 0.0936\n",
            "Epoch 6/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 5.5526 - accuracy: 0.0773 - val_loss: 5.9051 - val_accuracy: 0.0962\n",
            "Epoch 7/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 5.4432 - accuracy: 0.0924 - val_loss: 5.8752 - val_accuracy: 0.1030\n",
            "Epoch 8/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 5.3319 - accuracy: 0.1038 - val_loss: 5.8564 - val_accuracy: 0.1042\n",
            "Epoch 9/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 5.2215 - accuracy: 0.1144 - val_loss: 5.8349 - val_accuracy: 0.1106\n",
            "Epoch 10/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 5.1173 - accuracy: 0.1228 - val_loss: 5.8220 - val_accuracy: 0.1161\n",
            "Epoch 11/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 5.0157 - accuracy: 0.1321 - val_loss: 5.8141 - val_accuracy: 0.1172\n",
            "Epoch 12/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 4.9160 - accuracy: 0.1412 - val_loss: 5.8022 - val_accuracy: 0.1198\n",
            "Epoch 13/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 4.8174 - accuracy: 0.1458 - val_loss: 5.8141 - val_accuracy: 0.1183\n",
            "Epoch 14/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 4.7203 - accuracy: 0.1521 - val_loss: 5.8092 - val_accuracy: 0.1251\n",
            "Epoch 15/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 4.6259 - accuracy: 0.1605 - val_loss: 5.8193 - val_accuracy: 0.1227\n",
            "Epoch 16/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 4.5320 - accuracy: 0.1656 - val_loss: 5.8227 - val_accuracy: 0.1257\n",
            "Epoch 17/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 4.4375 - accuracy: 0.1729 - val_loss: 5.8454 - val_accuracy: 0.1270\n",
            "Epoch 18/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 4.3429 - accuracy: 0.1801 - val_loss: 5.8643 - val_accuracy: 0.1279\n",
            "Epoch 19/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 4.2484 - accuracy: 0.1847 - val_loss: 5.8914 - val_accuracy: 0.1227\n",
            "Epoch 20/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 4.1560 - accuracy: 0.1919 - val_loss: 5.9052 - val_accuracy: 0.1291\n",
            "Epoch 21/100\n",
            "166/166 [==============================] - 6s 39ms/step - loss: 4.0642 - accuracy: 0.1975 - val_loss: 5.9321 - val_accuracy: 0.1285\n",
            "Epoch 22/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 3.9724 - accuracy: 0.2052 - val_loss: 5.9569 - val_accuracy: 0.1281\n",
            "Epoch 23/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 3.8824 - accuracy: 0.2134 - val_loss: 5.9869 - val_accuracy: 0.1287\n",
            "Epoch 24/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 3.7928 - accuracy: 0.2257 - val_loss: 6.0186 - val_accuracy: 0.1289\n",
            "Epoch 25/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 3.7019 - accuracy: 0.2368 - val_loss: 6.0484 - val_accuracy: 0.1272\n",
            "Epoch 26/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 3.6152 - accuracy: 0.2492 - val_loss: 6.0836 - val_accuracy: 0.1255\n",
            "Epoch 27/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 3.5268 - accuracy: 0.2624 - val_loss: 6.1199 - val_accuracy: 0.1240\n",
            "Epoch 28/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 3.4430 - accuracy: 0.2773 - val_loss: 6.1514 - val_accuracy: 0.1268\n",
            "Epoch 29/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 3.3550 - accuracy: 0.2947 - val_loss: 6.1857 - val_accuracy: 0.1278\n",
            "Epoch 30/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 3.2702 - accuracy: 0.3074 - val_loss: 6.2264 - val_accuracy: 0.1276\n",
            "Epoch 31/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 3.1874 - accuracy: 0.3220 - val_loss: 6.2663 - val_accuracy: 0.1274\n",
            "Epoch 32/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 3.1062 - accuracy: 0.3388 - val_loss: 6.3030 - val_accuracy: 0.1242\n",
            "Epoch 33/100\n",
            "166/166 [==============================] - 6s 39ms/step - loss: 3.0276 - accuracy: 0.3577 - val_loss: 6.3388 - val_accuracy: 0.1242\n",
            "Epoch 34/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.9524 - accuracy: 0.3669 - val_loss: 6.3820 - val_accuracy: 0.1206\n",
            "Epoch 35/100\n",
            "166/166 [==============================] - 7s 39ms/step - loss: 2.8774 - accuracy: 0.3835 - val_loss: 6.4182 - val_accuracy: 0.1223\n",
            "Epoch 36/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.8054 - accuracy: 0.3993 - val_loss: 6.4700 - val_accuracy: 0.1174\n",
            "Epoch 37/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.7362 - accuracy: 0.4107 - val_loss: 6.5071 - val_accuracy: 0.1164\n",
            "Epoch 38/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.6659 - accuracy: 0.4235 - val_loss: 6.5531 - val_accuracy: 0.1157\n",
            "Epoch 39/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.6014 - accuracy: 0.4368 - val_loss: 6.6032 - val_accuracy: 0.1134\n",
            "Epoch 40/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.5388 - accuracy: 0.4495 - val_loss: 6.6390 - val_accuracy: 0.1168\n",
            "Epoch 41/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.4754 - accuracy: 0.4619 - val_loss: 6.6908 - val_accuracy: 0.1142\n",
            "Epoch 42/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 2.4159 - accuracy: 0.4746 - val_loss: 6.7422 - val_accuracy: 0.1104\n",
            "Epoch 43/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.3583 - accuracy: 0.4845 - val_loss: 6.7775 - val_accuracy: 0.1155\n",
            "Epoch 44/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.3009 - accuracy: 0.4992 - val_loss: 6.8360 - val_accuracy: 0.1117\n",
            "Epoch 45/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 2.2468 - accuracy: 0.5090 - val_loss: 6.8770 - val_accuracy: 0.1096\n",
            "Epoch 46/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 2.1917 - accuracy: 0.5207 - val_loss: 6.9299 - val_accuracy: 0.1091\n",
            "Epoch 47/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 2.1398 - accuracy: 0.5305 - val_loss: 6.9762 - val_accuracy: 0.1117\n",
            "Epoch 48/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.0908 - accuracy: 0.5409 - val_loss: 7.0258 - val_accuracy: 0.1106\n",
            "Epoch 49/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 2.0411 - accuracy: 0.5539 - val_loss: 7.0942 - val_accuracy: 0.1059\n",
            "Epoch 50/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.9930 - accuracy: 0.5613 - val_loss: 7.1347 - val_accuracy: 0.1051\n",
            "Epoch 51/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.9442 - accuracy: 0.5737 - val_loss: 7.1766 - val_accuracy: 0.1030\n",
            "Epoch 52/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.8995 - accuracy: 0.5850 - val_loss: 7.2216 - val_accuracy: 0.1012\n",
            "Epoch 53/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.8543 - accuracy: 0.5925 - val_loss: 7.2675 - val_accuracy: 0.1066\n",
            "Epoch 54/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.8100 - accuracy: 0.6053 - val_loss: 7.3372 - val_accuracy: 0.1059\n",
            "Epoch 55/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.7672 - accuracy: 0.6141 - val_loss: 7.3820 - val_accuracy: 0.1070\n",
            "Epoch 56/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.7241 - accuracy: 0.6255 - val_loss: 7.4242 - val_accuracy: 0.1045\n",
            "Epoch 57/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.6843 - accuracy: 0.6344 - val_loss: 7.4832 - val_accuracy: 0.1025\n",
            "Epoch 58/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.6437 - accuracy: 0.6435 - val_loss: 7.5414 - val_accuracy: 0.1008\n",
            "Epoch 59/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.6044 - accuracy: 0.6529 - val_loss: 7.5860 - val_accuracy: 0.1019\n",
            "Epoch 60/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.5672 - accuracy: 0.6614 - val_loss: 7.6376 - val_accuracy: 0.1015\n",
            "Epoch 61/100\n",
            "166/166 [==============================] - 7s 43ms/step - loss: 1.5288 - accuracy: 0.6692 - val_loss: 7.6892 - val_accuracy: 0.1004\n",
            "Epoch 62/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.4952 - accuracy: 0.6799 - val_loss: 7.7368 - val_accuracy: 0.1019\n",
            "Epoch 63/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.4567 - accuracy: 0.6882 - val_loss: 7.8016 - val_accuracy: 0.0998\n",
            "Epoch 64/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.4212 - accuracy: 0.6966 - val_loss: 7.8473 - val_accuracy: 0.0991\n",
            "Epoch 65/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.3878 - accuracy: 0.7055 - val_loss: 7.8914 - val_accuracy: 0.0995\n",
            "Epoch 66/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.3533 - accuracy: 0.7127 - val_loss: 7.9451 - val_accuracy: 0.0970\n",
            "Epoch 67/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.3199 - accuracy: 0.7222 - val_loss: 8.0073 - val_accuracy: 0.0993\n",
            "Epoch 68/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.2900 - accuracy: 0.7289 - val_loss: 8.0470 - val_accuracy: 0.0989\n",
            "Epoch 69/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.2563 - accuracy: 0.7374 - val_loss: 8.1182 - val_accuracy: 0.0978\n",
            "Epoch 70/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.2254 - accuracy: 0.7440 - val_loss: 8.1640 - val_accuracy: 0.0964\n",
            "Epoch 71/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.1949 - accuracy: 0.7508 - val_loss: 8.2205 - val_accuracy: 0.0989\n",
            "Epoch 72/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.1685 - accuracy: 0.7570 - val_loss: 8.2592 - val_accuracy: 0.0953\n",
            "Epoch 73/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.1364 - accuracy: 0.7678 - val_loss: 8.3058 - val_accuracy: 0.0966\n",
            "Epoch 74/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.1070 - accuracy: 0.7757 - val_loss: 8.3728 - val_accuracy: 0.0919\n",
            "Epoch 75/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.0819 - accuracy: 0.7799 - val_loss: 8.4363 - val_accuracy: 0.0913\n",
            "Epoch 76/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.0757 - accuracy: 0.7785 - val_loss: 8.4854 - val_accuracy: 0.0919\n",
            "Epoch 77/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 1.0359 - accuracy: 0.7925 - val_loss: 8.5354 - val_accuracy: 0.0917\n",
            "Epoch 78/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 1.0024 - accuracy: 0.8011 - val_loss: 8.5660 - val_accuracy: 0.0955\n",
            "Epoch 79/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.9768 - accuracy: 0.8074 - val_loss: 8.6323 - val_accuracy: 0.0902\n",
            "Epoch 80/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.9528 - accuracy: 0.8123 - val_loss: 8.6850 - val_accuracy: 0.0896\n",
            "Epoch 81/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.9290 - accuracy: 0.8175 - val_loss: 8.7542 - val_accuracy: 0.0915\n",
            "Epoch 82/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.9045 - accuracy: 0.8244 - val_loss: 8.7789 - val_accuracy: 0.0917\n",
            "Epoch 83/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.8797 - accuracy: 0.8303 - val_loss: 8.8452 - val_accuracy: 0.0895\n",
            "Epoch 84/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.8568 - accuracy: 0.8364 - val_loss: 8.9090 - val_accuracy: 0.0928\n",
            "Epoch 85/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.8345 - accuracy: 0.8410 - val_loss: 8.9666 - val_accuracy: 0.0902\n",
            "Epoch 86/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.8126 - accuracy: 0.8466 - val_loss: 9.0070 - val_accuracy: 0.0917\n",
            "Epoch 87/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.7940 - accuracy: 0.8508 - val_loss: 9.0592 - val_accuracy: 0.0923\n",
            "Epoch 88/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.7727 - accuracy: 0.8561 - val_loss: 9.1335 - val_accuracy: 0.0915\n",
            "Epoch 89/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.7503 - accuracy: 0.8624 - val_loss: 9.1744 - val_accuracy: 0.0893\n",
            "Epoch 90/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.7301 - accuracy: 0.8678 - val_loss: 9.2252 - val_accuracy: 0.0887\n",
            "Epoch 91/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.7106 - accuracy: 0.8730 - val_loss: 9.2750 - val_accuracy: 0.0896\n",
            "Epoch 92/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.6903 - accuracy: 0.8774 - val_loss: 9.3336 - val_accuracy: 0.0917\n",
            "Epoch 93/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.6721 - accuracy: 0.8812 - val_loss: 9.3848 - val_accuracy: 0.0896\n",
            "Epoch 94/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.6534 - accuracy: 0.8856 - val_loss: 9.4431 - val_accuracy: 0.0859\n",
            "Epoch 95/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.6335 - accuracy: 0.8910 - val_loss: 9.4954 - val_accuracy: 0.0878\n",
            "Epoch 96/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.6174 - accuracy: 0.8940 - val_loss: 9.5277 - val_accuracy: 0.0908\n",
            "Epoch 97/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.6002 - accuracy: 0.8979 - val_loss: 9.5998 - val_accuracy: 0.0889\n",
            "Epoch 98/100\n",
            "166/166 [==============================] - 7s 40ms/step - loss: 0.5810 - accuracy: 0.9026 - val_loss: 9.6568 - val_accuracy: 0.0891\n",
            "Epoch 99/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.5671 - accuracy: 0.9064 - val_loss: 9.7115 - val_accuracy: 0.0878\n",
            "Epoch 100/100\n",
            "166/166 [==============================] - 7s 41ms/step - loss: 0.5483 - accuracy: 0.9099 - val_loss: 9.7558 - val_accuracy: 0.0864\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc100c51e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg9WSEwYeMAH"
      },
      "source": [
        "## Create word to idx map using tokenizer.word_index\n",
        "\n",
        "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_mhL0J0eQku"
      },
      "source": [
        "# Complete the code to return next n words greedily\n",
        "def next_tokens(input_str, n): \n",
        "\tprint (\"Seed: \\n\",  input_str)\n",
        "\tfinal_string = \"\"\n",
        "\t\n",
        "\tfor i in range(n):\t\n",
        "\t\tx=tokenizer.texts_to_sequences([input_str])\n",
        "\t\t#print(x)\n",
        "\t\tprediction = model.predict(x , verbose=0)\n",
        "\t\t# get next word index. Use reverse_word_map to get the word\n",
        "\t\tindex = numpy.argmax(prediction)\n",
        "\t\t#print(index)\n",
        "\t\tnext_word = reverse_word_map[index]+\" \"\n",
        "\t\tfinal_string += next_word\n",
        "\t\tinput_str+= next_word\n",
        "\t\t#print(reverse_word_map[index])\n",
        "\treturn final_string "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4fIyAzKcaGX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "3b0f4a2a-3b86-4800-9464-5a19c2d72257"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "\n",
        "pattern = dataX[start]\n",
        "input_str = ' '.join([reverse_word_map[value] for value in pattern])\n",
        "\n",
        "next_tokens( input_str , 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: \n",
            " is of course not alice replied very readily but that’s because it stays the same year for such a long time together which is just the case with mine said the hatter alice felt dreadfully puzzled the hatter’s remark seemed to have no sort of meaning in it and yet\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you just as if you would you say what you '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsWLZi4qnFvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9876d2-1839-4440-8131-f59e4b47d211"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "\n",
        "# Use first 50 tokens from given input_str as input.(Use tokenizer to split to take first 50)\n",
        "print(next_tokens( input_str , 50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed: \n",
            " The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not  a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue  him or his sheep.\n",
            "said the king and alice thought to herself if you had been no use now but if you had been no time to be impertinent said alice and if you had been no one but she had wept if i must be impertinent said alice and if you had been \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_R5Tngo3_D"
      },
      "source": [
        "# Character based LSTM Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zZaHsejo57p"
      },
      "source": [
        "# User the preprocess data and create raw_text\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/corpus.txt\", \"rt\", encoding=\"utf-8\")\n",
        "corpus=f.read()\n",
        "raw_text = corpus.replace(\"_\", \"\")\n",
        "# create mapping of unique characters to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "\n",
        "char_to_int = {chars[i]:i for i in range(len(chars))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkVVDbump0Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a9203f-f3dd-4714-ffd9-f6d2cc0920e5"
      },
      "source": [
        "# Print the total characters and character vacob size\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"No. of characters: \",n_chars, \" Size of vocabulary: \", n_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of characters:  142037  Size of vocabulary:  71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aVserymqE1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0029c60b-5841-4999-a110-c5317520a31a"
      },
      "source": [
        "\n",
        "#Prepare dataset where the input is sequence of 100 characters and target is next character.\n",
        "\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):  \n",
        "    # Write code here\n",
        "    dataX.append([char_to_int[i] for i in raw_text[i:(i+seq_length)]])\n",
        "    dataY.append(char_to_int[raw_text[i+seq_length]])\n",
        "\n",
        "\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  141937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic4yf4hNqc7T"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.array(dataX)\n",
        "\n",
        "# one hot encode the output variable\n",
        "dataY = numpy.array(dataY)\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvawnjFVqhMi"
      },
      "source": [
        "embedding_dim =100\n",
        "max_length =100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek5DqNeTqkAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f8c7a7-4046-4702-d498-42994cfe995a"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "model = Sequential()\n",
        "model.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 100, 100)          7100      \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 256)               365568    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 71)                18247     \n",
            "=================================================================\n",
            "Total params: 390,915\n",
            "Trainable params: 390,915\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFoStJIOqpM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675aa517-fa74-42cd-d4e3-22e0eefe5fb4"
      },
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1109/1109 [==============================] - 47s 43ms/step - loss: 2.3382\n",
            "Epoch 2/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.8398\n",
            "Epoch 3/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.6434\n",
            "Epoch 4/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.5216\n",
            "Epoch 5/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.4359\n",
            "Epoch 6/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.3676\n",
            "Epoch 7/20\n",
            "1109/1109 [==============================] - 47s 43ms/step - loss: 1.3136\n",
            "Epoch 8/20\n",
            "1109/1109 [==============================] - 47s 43ms/step - loss: 1.2666\n",
            "Epoch 9/20\n",
            "1109/1109 [==============================] - 47s 43ms/step - loss: 1.2261\n",
            "Epoch 10/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.1914\n",
            "Epoch 11/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.1610\n",
            "Epoch 12/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.1297\n",
            "Epoch 13/20\n",
            "1109/1109 [==============================] - 47s 43ms/step - loss: 1.1031\n",
            "Epoch 14/20\n",
            "1109/1109 [==============================] - 47s 43ms/step - loss: 1.0783\n",
            "Epoch 15/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 1.0516\n",
            "Epoch 16/20\n",
            "1109/1109 [==============================] - 47s 43ms/step - loss: 1.0308\n",
            "Epoch 17/20\n",
            "1109/1109 [==============================] - 47s 43ms/step - loss: 1.0111\n",
            "Epoch 18/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 0.9888\n",
            "Epoch 19/20\n",
            "1109/1109 [==============================] - 48s 43ms/step - loss: 0.9742\n",
            "Epoch 20/20\n",
            "1109/1109 [==============================] - 47s 42ms/step - loss: 0.9542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc159887f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekXYhzeUrGaq"
      },
      "source": [
        "#implement mapping of integer to character\n",
        "int_to_char = {i:chars[i] for i in range(len(chars))}\n",
        "#print(int_to_char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OixPrw6vq15j"
      },
      "source": [
        "\n",
        "\n",
        "# Complete the code to return next n words greedily\n",
        "def predict_next_100_chars(pattern, x): \t\n",
        "\tfinal_string = \"\"\n",
        "\tencoded_str=np.reshape([char_to_int[ch] for ch in pattern], (1, len(pattern)))\n",
        "\tfor i in range(x):\t\t\n",
        "\t\tprediction = model.predict(encoded_str , verbose=0)\t\t\t\t\n",
        "\t\tindex = numpy.argmax(prediction)\t\t\t\n",
        "\t\tnext_char = int_to_char[index]\n",
        "\t\tfinal_string += next_char\t\t\n",
        "\t\tencoded_str = np.append(encoded_str, np.array([[index]]), axis=1)\t\n",
        "\treturn final_string \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHH_I5QiUxnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ee1760-019c-4209-b74e-155b7e64547e"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "input_str = ''.join([int_to_char[value] for value in pattern])\n",
        "\n",
        "print(predict_next_100_chars(input_str,200))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"embedding_3_input:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 101).\n",
            "order,” said the King, “and the moral of that is—‘The more than the executioner everything I’ve to talk about it,” said the King, “and the moral of that is—‘The more than the executioner everything I’\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iutpuJAgrgU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0e4233-da67-4fbb-8346-cbc5f81ee4f2"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "\n",
        " # Use first 100 characeters from given input_str as input and generate next 200 characters.\n",
        "\n",
        " \n",
        "print(predict_next_100_chars(input_str,200))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "“There’s no more than the executioner everything I’ve to say that is—‘The more than the moral of that is—‘The more than the executioner everything I’ve to talk about it,” said the King, “and the mora\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8shbcukr0tJ"
      },
      "source": [
        "## Character based LSTM Model 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWBPCrTdr46U"
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
        "model1.add(LSTM(256, input_shape=(X.shape[1], embedding_dim),return_sequences=True))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(LSTM(256))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(Dense(y.shape[1], activation='softmax'))\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZxrtjFIr63L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a10cb92a-b86a-4751-8b0e-d1529fbb8bb2"
      },
      "source": [
        "model1.fit(X, y, epochs=20, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 2.1795\n",
            "Epoch 2/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 1.6136\n",
            "Epoch 3/20\n",
            "2218/2218 [==============================] - 150s 68ms/step - loss: 1.4277\n",
            "Epoch 4/20\n",
            "2218/2218 [==============================] - 150s 68ms/step - loss: 1.3197\n",
            "Epoch 5/20\n",
            "2218/2218 [==============================] - 150s 68ms/step - loss: 1.2453\n",
            "Epoch 6/20\n",
            "2218/2218 [==============================] - 150s 68ms/step - loss: 1.1927\n",
            "Epoch 7/20\n",
            "2218/2218 [==============================] - 150s 67ms/step - loss: 1.1459\n",
            "Epoch 8/20\n",
            "2218/2218 [==============================] - 150s 67ms/step - loss: 1.1105\n",
            "Epoch 9/20\n",
            "2218/2218 [==============================] - 150s 68ms/step - loss: 1.0747\n",
            "Epoch 10/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 1.0495\n",
            "Epoch 11/20\n",
            "2218/2218 [==============================] - 150s 68ms/step - loss: 1.0209\n",
            "Epoch 12/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 0.9976\n",
            "Epoch 13/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 0.9780\n",
            "Epoch 14/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 0.9594\n",
            "Epoch 15/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 0.9424\n",
            "Epoch 16/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 0.9295\n",
            "Epoch 17/20\n",
            "2218/2218 [==============================] - 150s 68ms/step - loss: 0.9172\n",
            "Epoch 18/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 0.9021\n",
            "Epoch 19/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 0.8915\n",
            "Epoch 20/20\n",
            "2218/2218 [==============================] - 151s 68ms/step - loss: 0.8789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc0fa077e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6RTfHN6r8wh"
      },
      "source": [
        "# Generate the sequence similar to above methods\n",
        "\n",
        "\n",
        "\n",
        "def predict_next_100_chars(pattern, x): \t\n",
        "\tfinal_string = \"\"\n",
        "\tencoded_str=np.reshape([char_to_int[ch] for ch in pattern], (1, len(pattern)))\n",
        "\tfor i in range(x):\t\t\n",
        "\t\tprediction = model1.predict(encoded_str , verbose=0)\t\t\t\t\n",
        "\t\tindex = numpy.argmax(prediction)\t\t\t\n",
        "\t\tnext_char = int_to_char[index]\n",
        "\t\tfinal_string += next_char\t\t\n",
        "\t\tencoded_str = np.append(encoded_str, np.array([[index]]), axis=1)\t\n",
        "\treturn final_string \n",
        "\n",
        "\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKJXZ4tYmL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4151f0c6-f75f-4e00-c722-d4a80d9790ca"
      },
      "source": [
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "input_str = ''.join([int_to_char[value] for value in pattern])\n",
        "\n",
        "print(predict_next_100_chars(input_str,200))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " the sort of deal of the song, and then I must be a little sisters as the Queen of the soldiers,” said the Mock Turtle.\n",
            "“I don’t know what they’re saying to herself, and then I must be a little sister\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyRCfLe5YmL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5815976b-92e6-4c77-db08-e6f62202b4d5"
      },
      "source": [
        "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
        " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
        " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
        " him or his sheep.\"\n",
        "\n",
        " # Use first 100 characeters from given input_str as input and generate next 200 characters.\n",
        "\n",
        " \n",
        "print(predict_next_100_chars(input_str,200))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "” “I don’t know what they’re saying to herself, and then I must be a little sisters as the Queen of the soldiers,” said the Mock Turtle.\n",
            "“I don’t know what they’re saying to herself, and then I must b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPrjxjoNsaQC"
      },
      "source": [
        "# Performance of the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW5_IlBeZAPq"
      },
      "source": [
        "**Question:** What are the observations based on the model(all) outputs on train data(in domain) vs unseen data(out of domain) ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YryzP1upZv5h"
      },
      "source": [
        "\n",
        "Model seems to predict better for text based on the corpus. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBzCD0I0Z3uP"
      },
      "source": [
        "**Question:** What was observed in the outputs of char LSTM model1 vs char LSTM model2 ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUHxHmXZaNdn"
      },
      "source": [
        "**Answer:**\n",
        "Character based model in LSTM model2 seems to overfit more than model1 on observing the repeatition in the sequence of words generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2N-l6VSshhC"
      },
      "source": [
        "# Transformer based language model (Bert)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67c13Yx9sgiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5d2733-79e0-449b-f062-9489019d0d6e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 31.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gF2nncdsqcx"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import string\n",
        "from transformers import BertTokenizer, BertForMaskedLM"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tppcQN298Qs"
      },
      "source": [
        "def load_model(model_name):\n",
        "  try:\n",
        "    if model_name.lower() == \"bert\":\n",
        "      bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "      bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n",
        "      return bert_tokenizer,bert_model\n",
        "  except Exception as e:\n",
        "    pass"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOcQKOGG-H8F"
      },
      "source": [
        "def decode(tokenizer, pred_idx, top_clean):\n",
        "  ignore_tokens = string.punctuation + '[PAD]'\n",
        "  tokens = []\n",
        "  for w in pred_idx:\n",
        "    token = ''.join(tokenizer.decode(w).split())\n",
        "    if token not in ignore_tokens:\n",
        "      tokens.append(token.replace('##', ''))\n",
        "  return '\\n'.join(tokens[:top_clean])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQkjSmGv-TxO"
      },
      "source": [
        "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
        "  text_sentence = text_sentence.replace('<mask>', tokenizer.mask_token)\n",
        "  # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
        "  if tokenizer.mask_token == text_sentence.split()[-1]:\n",
        "    text_sentence += ' .'\n",
        "  input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
        "  mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
        "  return input_ids, mask_idx"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFWOHB5n-ZQh"
      },
      "source": [
        "def get_all_predictions(text_sentence, top_clean=5):\n",
        "  input_ids, mask_idx = encode(bert_tokenizer, text_sentence)\n",
        "  with torch.no_grad():\n",
        "    predict = bert_model(input_ids)[0]\n",
        "    print(predict.shape)\n",
        "    \n",
        "    bert = decode(bert_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n",
        "  return {'bert': bert}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lQHVc0l-igk"
      },
      "source": [
        "def get_prediction_eos(input_text):\n",
        "  try:\n",
        "    input_text += ' <mask>'\n",
        "    res = get_all_predictions(input_text, top_clean=int(top_k))\n",
        "    return res\n",
        "  except Exception as error:\n",
        "    pass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQZ-swMD9lyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb5a7ed-6a7b-424b-addf-5167dd576750"
      },
      "source": [
        "# Below code predicts the next top_k words.  \n",
        "top_k= 3\n",
        "print('Predict next top', top_k, ' words')\n",
        "model_name = 'BERT'\n",
        "bert_tokenizer, bert_model  = load_model(model_name) \n",
        "input_text = \"Will you be my \" ### GIVE YOUR INPUT STRING HERE\n",
        "res = get_prediction_eos(input_text)\n",
        "answer = []\n",
        "print(res['bert'].split(\"\\n\"))\n",
        "for i in res['bert'].split(\"\\n\"):\n",
        "  answer.append(i)\n",
        "  answer_as_string = \" \".join(answer)\n",
        "\n",
        "print(answer_as_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict next  3  words\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8, 30522])\n",
            "['wife', 'friend', 'husband']\n",
            "wife friend husband\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRsd9CaGZkNs",
        "outputId": "9b9ab92c-50d3-44e4-ba9c-3e78145e5a35"
      },
      "source": [
        "#You can modify the above code to get next n words using top_k=1 and greedily decoding it.\n",
        "top_k= 1\n",
        "\n",
        "model_name = 'BERT'\n",
        "bert_tokenizer, bert_model  = load_model(model_name) \n",
        "input_text = \"I said you \" ### GIVE YOUR INPUT STRING HERE\n",
        "print(\"_____________________________________\")\n",
        "print(\"Input:\")\n",
        "print(input_text)\n",
        "print(\"_____________________________________\")\n",
        "k=5  #predict next k words\n",
        "print(\"k:\",k)\n",
        "print(\"_____________________________________\")\n",
        "for i in range(k):\n",
        "  res = get_prediction_eos(input_text)\n",
        "  input_text+=res['bert']\n",
        "  input_text+=\" \"\n",
        "print(\"_____________________________________\")\n",
        "print(\"Output:\")\n",
        "print(input_text)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_____________________________________\n",
            "Input:\n",
            "I said you \n",
            "_____________________________________\n",
            "k: 5\n",
            "_____________________________________\n",
            "torch.Size([1, 7, 30522])\n",
            "torch.Size([1, 8, 30522])\n",
            "torch.Size([1, 9, 30522])\n",
            "torch.Size([1, 10, 30522])\n",
            "torch.Size([1, 11, 30522])\n",
            "_____________________________________\n",
            "Output:\n",
            "I said you would come back tomorrow night \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}